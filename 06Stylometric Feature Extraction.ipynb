{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1802f673",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-27T18:45:11.245798Z",
     "end_time": "2023-11-27T18:45:14.605611Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 160\n",
    "\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "import features as util\n",
    "from features import language_tool_python\n",
    "from preprocessing import tokenize, impute_missing\n",
    "from raw_utils import save_to_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b9f5f8",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2af55909",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-27T18:45:14.609607Z",
     "end_time": "2023-11-27T18:45:14.617956Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path\n",
    "cwd = os.getcwd()\n",
    "csv_path = os.path.join(cwd, 'data/csv/')\n",
    "\n",
    "train_text = ['train_balanced_text.csv', 'train_imbalanced_text.csv']\n",
    "test_text = ['test_balanced_text.csv', 'test_imbalanced_text.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac3b15",
   "metadata": {},
   "source": [
    "#### Email Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b48a6eaa",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-27T18:45:14.621946Z",
     "end_time": "2023-11-27T18:45:14.729959Z"
    }
   },
   "outputs": [],
   "source": [
    "train_balanced_text = pd.read_csv(os.path.join(csv_path, train_text[0]), index_col=0)\n",
    "test_balanced_text = pd.read_csv(os.path.join(csv_path, test_text[0]), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e93526e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-27T18:45:14.733958Z",
     "end_time": "2023-11-27T18:45:15.356922Z"
    }
   },
   "outputs": [],
   "source": [
    "train_imbalanced_text = pd.read_csv(os.path.join(csv_path, train_text[1]), index_col=0)\n",
    "test_imbalanced_text = pd.read_csv(os.path.join(csv_path, test_text[1]), index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27680c4a",
   "metadata": {},
   "source": [
    "After the preprocessing, the data look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f271a3a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-27T18:45:15.360823Z",
     "end_time": "2023-11-27T18:45:15.376810Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     id  \\\n0  2734   \n1  2703   \n2  1702   \n3  1627   \n4  2636   \n\n                                                                                                                                                              body  \\\n0                 it was fun.  i had a blast.  my liver is still recovering?  did you hear \\ngrandma won a truck?  that is amazing.  what has been going on there?   \n1  Attention!!!\\n\\nYour Luno Account requires Profile Update due to SSL server upgrade.\\n\\nClick here to Upgrade now. \\n\\nFailure to upgrade with 48 hours will...   \n2  The following reports have been waiting for your approval for more than 4 days.  Please review.\\n\\nOwner: Stephen C Hall\\nReport Name: Steve C. Hall\\nDays I...   \n3  Spoke with Ben Glisan re our conversation on retail credit and concluded that \\nwe should meet with some banks that specialize in this area. Can you prepare...   \n4  <table style=\"background-color: #eaeaea; margin-top: 0;\" border=\"0\" width=\"100%\" cellspacing=\"0\" cellpadding=\"0\">\\n<tbody>\\n<tr>\\n<td align=\"center\">\\n<tabl...   \n\n   class  \n0  False  \n1   True  \n2  False  \n3  False  \n4   True  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>body</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2734</td>\n      <td>it was fun.  i had a blast.  my liver is still recovering?  did you hear \\ngrandma won a truck?  that is amazing.  what has been going on there?</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2703</td>\n      <td>Attention!!!\\n\\nYour Luno Account requires Profile Update due to SSL server upgrade.\\n\\nClick here to Upgrade now. \\n\\nFailure to upgrade with 48 hours will...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1702</td>\n      <td>The following reports have been waiting for your approval for more than 4 days.  Please review.\\n\\nOwner: Stephen C Hall\\nReport Name: Steve C. Hall\\nDays I...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1627</td>\n      <td>Spoke with Ben Glisan re our conversation on retail credit and concluded that \\nwe should meet with some banks that specialize in this area. Can you prepare...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2636</td>\n      <td>&lt;table style=\"background-color: #eaeaea; margin-top: 0;\" border=\"0\" width=\"100%\" cellspacing=\"0\" cellpadding=\"0\"&gt;\\n&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td align=\"center\"&gt;\\n&lt;tabl...</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_balanced_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948acf1c",
   "metadata": {},
   "source": [
    "#### Email Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e28707c",
   "metadata": {},
   "source": [
    "Since the .csv files with the already tokenized emails have been subject to further preprocessing like lemmatization and stopword removal, a simple tokenization (at word and sentence level) will also be run here for the purposes of feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c970d2c6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-27T18:45:15.376810Z",
     "end_time": "2023-11-27T18:46:02.565440Z"
    }
   },
   "outputs": [],
   "source": [
    "train_balanced_text['tokens'] = train_balanced_text['body'].apply(tokenize)\n",
    "test_balanced_text['tokens'] = test_balanced_text['body'].apply(tokenize)\n",
    "train_imbalanced_text['tokens'] = train_imbalanced_text['body'].apply(tokenize)\n",
    "test_imbalanced_text['tokens'] = test_imbalanced_text['body'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e357899",
   "metadata": {},
   "source": [
    "Note that for the sentence-level tokenization, `nltk.sent_tokenization` is used, so any sentences separated by a newline without punctuation will be considered simply as wrapped text, and not new, different sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2a3ad87",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-27T18:46:02.569358Z",
     "end_time": "2023-11-27T18:46:09.385120Z"
    }
   },
   "outputs": [],
   "source": [
    "train_balanced_text['sentences'] = train_balanced_text['body'].apply(sent_tokenize)\n",
    "test_balanced_text['sentences'] = test_balanced_text['body'].apply(sent_tokenize)\n",
    "train_imbalanced_text['sentences'] = train_imbalanced_text['body'].apply(sent_tokenize)\n",
    "test_imbalanced_text['sentences'] = test_imbalanced_text['body'].apply(sent_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fdcf9b",
   "metadata": {},
   "source": [
    "# Stylometric Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f9c06",
   "metadata": {},
   "source": [
    "Useful markers of whether an email is phishing or not should stem from the writing style of the author.<br>\n",
    "With this in mind, several features that have been previously used in the literature will be extracted, in order to be compared and combined with the text-only baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe758756",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-27T18:46:09.385120Z",
     "end_time": "2023-11-27T18:46:09.417238Z"
    }
   },
   "outputs": [],
   "source": [
    "train_balanced_style = train_balanced_text[['id', 'class']].copy()\n",
    "test_balanced_style = test_balanced_text[['id', 'class']].copy()\n",
    "train_imbalanced_style = train_imbalanced_text[['id', 'class']].copy()\n",
    "test_imbalanced_style = test_imbalanced_text[['id', 'class']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad6019e",
   "metadata": {},
   "source": [
    "### Simple Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bbfcb7",
   "metadata": {},
   "source": [
    "The simplest kind of features would be simple counts of various parts of the emails, like characters and words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1f771bf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-27T18:46:09.421223Z",
     "end_time": "2023-11-27T18:46:09.545117Z"
    }
   },
   "outputs": [],
   "source": [
    "train_balanced_style['num_chars'] = train_balanced_text['body'].apply(util.count_chars)\n",
    "train_balanced_style['num_newlines'] = train_balanced_text['body'].apply(util.count_newlines)\n",
    "train_balanced_style['num_special_chars'] = train_balanced_text['body'].apply(util.count_special_chars)\n",
    "train_balanced_style['num_words'] = train_balanced_text['tokens'].apply(util.count_words)\n",
    "train_balanced_style['num_unique_words'] = train_balanced_text['tokens'].apply(util.count_unique_words)\n",
    "train_balanced_style['sentences'] = train_balanced_text['sentences'].apply(util.count_sentences)\n",
    "train_balanced_style[['num_sentences', 'num_upper_sentences', 'num_lower_sentences']] = pd.DataFrame(train_balanced_style['sentences'].tolist(), index=train_balanced_style.index)\n",
    "train_balanced_style = train_balanced_style.drop('sentences', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98b11e54",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-27T18:46:09.545117Z",
     "end_time": "2023-11-27T18:46:09.593412Z"
    }
   },
   "outputs": [],
   "source": [
    "test_balanced_style['num_chars'] = test_balanced_text['body'].apply(util.count_chars)\n",
    "test_balanced_style['num_newlines'] = test_balanced_text['body'].apply(util.count_newlines)\n",
    "test_balanced_style['num_special_chars'] = test_balanced_text['body'].apply(util.count_special_chars)\n",
    "test_balanced_style['num_words'] = test_balanced_text['tokens'].apply(util.count_words)\n",
    "test_balanced_style['num_unique_words'] = test_balanced_text['tokens'].apply(util.count_unique_words)\n",
    "test_balanced_style['sentences'] = test_balanced_text['sentences'].apply(util.count_sentences)\n",
    "test_balanced_style[['num_sentences', 'num_upper_sentences', 'num_lower_sentences']] = pd.DataFrame(test_balanced_style['sentences'].tolist(), index=test_balanced_style.index)\n",
    "test_balanced_style = test_balanced_style.drop('sentences', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "058d0e08",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-27T18:46:09.609399Z",
     "end_time": "2023-11-27T18:46:10.349381Z"
    }
   },
   "outputs": [],
   "source": [
    "train_imbalanced_style['num_chars'] = train_imbalanced_text['body'].apply(util.count_chars)\n",
    "train_imbalanced_style['num_newlines'] = train_imbalanced_text['body'].apply(util.count_newlines)\n",
    "train_imbalanced_style['num_special_chars'] = train_imbalanced_text['body'].apply(util.count_special_chars)\n",
    "train_imbalanced_style['num_words'] = train_imbalanced_text['tokens'].apply(util.count_words)\n",
    "train_imbalanced_style['num_unique_words'] = train_imbalanced_text['tokens'].apply(util.count_unique_words)\n",
    "train_imbalanced_style['sentences'] = train_imbalanced_text['sentences'].apply(util.count_sentences)\n",
    "train_imbalanced_style[['num_sentences', 'num_upper_sentences', 'num_lower_sentences']] = pd.DataFrame(train_imbalanced_style['sentences'].tolist(), index=train_imbalanced_style.index)\n",
    "train_imbalanced_style = train_imbalanced_style.drop('sentences', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88450b24",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-27T18:46:10.398299Z",
     "end_time": "2023-11-27T18:46:10.526518Z"
    }
   },
   "outputs": [],
   "source": [
    "test_imbalanced_style['num_chars'] = test_imbalanced_text['body'].apply(util.count_chars)\n",
    "test_imbalanced_style['num_newlines'] = test_imbalanced_text['body'].apply(util.count_newlines)\n",
    "test_imbalanced_style['num_special_chars'] = test_imbalanced_text['body'].apply(util.count_special_chars)\n",
    "test_imbalanced_style['num_words'] = test_imbalanced_text['tokens'].apply(util.count_words)\n",
    "test_imbalanced_style['num_unique_words'] = test_imbalanced_text['tokens'].apply(util.count_unique_words)\n",
    "test_imbalanced_style['sentences'] = test_imbalanced_text['sentences'].apply(util.count_sentences)\n",
    "test_imbalanced_style[['num_sentences', 'num_upper_sentences', 'num_lower_sentences']] = pd.DataFrame(test_imbalanced_style['sentences'].tolist(), index=test_imbalanced_style.index)\n",
    "test_imbalanced_style = test_imbalanced_style.drop('sentences', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefcdc73",
   "metadata": {},
   "source": [
    "## Word Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d89241",
   "metadata": {},
   "source": [
    "Another category of features are those that related to the size of words, like the average size and counts or frequencies of smaller or bigger words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9586db5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-27T18:46:10.530517Z",
     "end_time": "2023-11-27T18:46:10.766266Z"
    }
   },
   "outputs": [],
   "source": [
    "train_balanced_style['avg_word_size'] = train_balanced_text['tokens'].apply(util.average_word_length)\n",
    "train_balanced_style['small_words'] = train_balanced_text['tokens'].apply(util.small_words)\n",
    "train_balanced_style[['num_small_words', 'freq_small_words']] = pd.DataFrame(train_balanced_style['small_words'].tolist(), index=train_balanced_style.index)\n",
    "train_balanced_style['big_words'] = train_balanced_text['tokens'].apply(util.big_words)\n",
    "train_balanced_style[['num_big_words', 'freq_big_words']] = pd.DataFrame(train_balanced_style['big_words'].tolist(), index=train_balanced_style.index)\n",
    "train_balanced_style['huge_words'] = train_balanced_text['tokens'].apply(util.huge_words)\n",
    "train_balanced_style[['num_huge_words', 'freq_huge_words']] = pd.DataFrame(train_balanced_style['huge_words'].tolist(), index=train_balanced_style.index)\n",
    "train_balanced_style = train_balanced_style.drop(['small_words', 'big_words', 'huge_words'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bbded68",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-27T18:46:10.770179Z",
     "end_time": "2023-11-27T18:46:10.845684Z"
    }
   },
   "outputs": [],
   "source": [
    "test_balanced_style['avg_word_size'] = test_balanced_text['tokens'].apply(util.average_word_length)\n",
    "test_balanced_style['small_words'] = test_balanced_text['tokens'].apply(util.small_words)\n",
    "test_balanced_style[['num_small_words', 'freq_small_words']] = pd.DataFrame(test_balanced_style['small_words'].tolist(), index=test_balanced_style.index)\n",
    "test_balanced_style['big_words'] = test_balanced_text['tokens'].apply(util.big_words)\n",
    "test_balanced_style[['num_big_words', 'freq_big_words']] = pd.DataFrame(test_balanced_style['big_words'].tolist(), index=test_balanced_style.index)\n",
    "test_balanced_style['huge_words'] = test_balanced_text['tokens'].apply(util.huge_words)\n",
    "test_balanced_style[['num_huge_words', 'freq_huge_words']] = pd.DataFrame(test_balanced_style['huge_words'].tolist(), index=test_balanced_style.index)\n",
    "test_balanced_style = test_balanced_style.drop(['small_words', 'big_words', 'huge_words'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54525efa",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-27T18:46:10.849753Z",
     "end_time": "2023-11-27T18:46:12.027941Z"
    }
   },
   "outputs": [],
   "source": [
    "train_imbalanced_style['avg_word_size'] = train_imbalanced_text['tokens'].apply(util.average_word_length)\n",
    "train_imbalanced_style['small_words'] = train_imbalanced_text['tokens'].apply(util.small_words)\n",
    "train_imbalanced_style[['num_small_words', 'freq_small_words']] = pd.DataFrame(train_imbalanced_style['small_words'].tolist(), index=train_imbalanced_style.index)\n",
    "train_imbalanced_style['big_words'] = train_imbalanced_text['tokens'].apply(util.big_words)\n",
    "train_imbalanced_style[['num_big_words', 'freq_big_words']] = pd.DataFrame(train_imbalanced_style['big_words'].tolist(), index=train_imbalanced_style.index)\n",
    "train_imbalanced_style['huge_words'] = train_imbalanced_text['tokens'].apply(util.huge_words)\n",
    "train_imbalanced_style[['num_huge_words', 'freq_huge_words']] = pd.DataFrame(train_imbalanced_style['huge_words'].tolist(), index=train_imbalanced_style.index)\n",
    "train_imbalanced_style = train_imbalanced_style.drop(['small_words', 'big_words', 'huge_words'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44b712d7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-27T18:46:12.031937Z",
     "end_time": "2023-11-27T18:46:12.386057Z"
    }
   },
   "outputs": [],
   "source": [
    "test_imbalanced_style['avg_word_size'] = test_imbalanced_text['tokens'].apply(util.average_word_length)\n",
    "test_imbalanced_style['small_words'] = test_imbalanced_text['tokens'].apply(util.small_words)\n",
    "test_imbalanced_style[['num_small_words', 'freq_small_words']] = pd.DataFrame(test_imbalanced_style['small_words'].tolist(), index=test_imbalanced_style.index)\n",
    "test_imbalanced_style['big_words'] = test_imbalanced_text['tokens'].apply(util.big_words)\n",
    "test_imbalanced_style[['num_big_words', 'freq_big_words']] = pd.DataFrame(test_imbalanced_style['big_words'].tolist(), index=test_imbalanced_style.index)\n",
    "test_imbalanced_style['huge_words'] = test_imbalanced_text['tokens'].apply(util.huge_words)\n",
    "test_imbalanced_style[['num_huge_words', 'freq_huge_words']] = pd.DataFrame(test_imbalanced_style['huge_words'].tolist(), index=test_imbalanced_style.index)\n",
    "test_imbalanced_style = test_imbalanced_style.drop(['small_words', 'big_words', 'huge_words'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a91765",
   "metadata": {},
   "source": [
    "## Sentence Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d0aedb",
   "metadata": {},
   "source": [
    "Another set of features could be related to various simple statistics about the size of a sentence, using both characters and words as a unit of size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02da8f07",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-27T18:46:12.430019Z",
     "end_time": "2023-11-27T18:46:34.815526Z"
    }
   },
   "outputs": [],
   "source": [
    "train_balanced_style['avg_sent_size'] = train_balanced_text['sentences'].apply(util.average_sentence_length)\n",
    "train_balanced_style[['avg_sentence_chars', 'avg_sentence_words']] = pd.DataFrame(train_balanced_style['avg_sent_size'].tolist(), index=train_balanced_style.index)\n",
    "train_balanced_style['std_sent_size'] = train_balanced_text['sentences'].apply(util.std_sentence_length)\n",
    "train_balanced_style[['std_sentence_chars', 'std_sentence_words']] = pd.DataFrame(train_balanced_style['std_sent_size'].tolist(), index=train_balanced_style.index)\n",
    "train_balanced_style['min_sent_size'] = train_balanced_text['sentences'].apply(util.min_sentence_length)\n",
    "train_balanced_style[['min_sentence_chars', 'min_sentence_words']] = pd.DataFrame(train_balanced_style['min_sent_size'].tolist(), index=train_balanced_style.index)\n",
    "train_balanced_style['max_sent_size'] = train_balanced_text['sentences'].apply(util.max_sentence_length)\n",
    "train_balanced_style[['max_sentence_chars', 'max_sentence_words']] = pd.DataFrame(train_balanced_style['max_sent_size'].tolist(), index=train_balanced_style.index)\n",
    "train_balanced_style = train_balanced_style.drop(['avg_sent_size', 'std_sent_size', 'min_sent_size', 'max_sent_size'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "073706f7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-27T18:46:34.819124Z",
     "end_time": "2023-11-27T18:46:39.245972Z"
    }
   },
   "outputs": [],
   "source": [
    "test_balanced_style['avg_sent_size'] = test_balanced_text['sentences'].apply(util.average_sentence_length)\n",
    "test_balanced_style[['avg_sentence_chars', 'avg_sentence_words']] = pd.DataFrame(test_balanced_style['avg_sent_size'].tolist(), index=test_balanced_style.index)\n",
    "test_balanced_style['std_sent_size'] = test_balanced_text['sentences'].apply(util.std_sentence_length)\n",
    "test_balanced_style[['std_sentence_chars', 'std_sentence_words']] = pd.DataFrame(test_balanced_style['std_sent_size'].tolist(), index=test_balanced_style.index)\n",
    "test_balanced_style['min_sent_size'] = test_balanced_text['sentences'].apply(util.min_sentence_length)\n",
    "test_balanced_style[['min_sentence_chars', 'min_sentence_words']] = pd.DataFrame(test_balanced_style['min_sent_size'].tolist(), index=test_balanced_style.index)\n",
    "test_balanced_style['max_sent_size'] = test_balanced_text['sentences'].apply(util.max_sentence_length)\n",
    "test_balanced_style[['max_sentence_chars', 'max_sentence_words']] = pd.DataFrame(test_balanced_style['max_sent_size'].tolist(), index=test_balanced_style.index)\n",
    "test_balanced_style = test_balanced_style.drop(['avg_sent_size', 'std_sent_size', 'min_sent_size', 'max_sent_size'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f66ec9e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-27T18:46:39.249935Z",
     "end_time": "2023-11-27T18:48:47.187400Z"
    }
   },
   "outputs": [],
   "source": [
    "train_imbalanced_style['avg_sent_size'] = train_imbalanced_text['sentences'].apply(util.average_sentence_length)\n",
    "train_imbalanced_style[['avg_sentence_chars', 'avg_sentence_words']] = pd.DataFrame(train_imbalanced_style['avg_sent_size'].tolist(), index=train_imbalanced_style.index)\n",
    "train_imbalanced_style['std_sent_size'] = train_imbalanced_text['sentences'].apply(util.std_sentence_length)\n",
    "train_imbalanced_style[['std_sentence_chars', 'std_sentence_words']] = pd.DataFrame(train_imbalanced_style['std_sent_size'].tolist(), index=train_imbalanced_style.index)\n",
    "train_imbalanced_style['min_sent_size'] = train_imbalanced_text['sentences'].apply(util.min_sentence_length)\n",
    "train_imbalanced_style[['min_sentence_chars', 'min_sentence_words']] = pd.DataFrame(train_imbalanced_style['min_sent_size'].tolist(), index=train_imbalanced_style.index)\n",
    "train_imbalanced_style['max_sent_size'] = train_imbalanced_text['sentences'].apply(util.max_sentence_length)\n",
    "train_imbalanced_style[['max_sentence_chars', 'max_sentence_words']] = pd.DataFrame(train_imbalanced_style['max_sent_size'].tolist(), index=train_imbalanced_style.index)\n",
    "train_imbalanced_style = train_imbalanced_style.drop(['avg_sent_size', 'std_sent_size', 'min_sent_size', 'max_sent_size'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20e5e04f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-27T18:48:47.234659Z",
     "end_time": "2023-11-27T18:49:21.012188Z"
    }
   },
   "outputs": [],
   "source": [
    "test_imbalanced_style['avg_sent_size'] = test_imbalanced_text['sentences'].apply(util.average_sentence_length)\n",
    "test_imbalanced_style[['avg_sentence_chars', 'avg_sentence_words']] = pd.DataFrame(test_imbalanced_style['avg_sent_size'].tolist(), index=test_imbalanced_style.index)\n",
    "test_imbalanced_style['std_sent_size'] = test_imbalanced_text['sentences'].apply(util.std_sentence_length)\n",
    "test_imbalanced_style[['std_sentence_chars', 'std_sentence_words']] = pd.DataFrame(test_imbalanced_style['std_sent_size'].tolist(), index=test_imbalanced_style.index)\n",
    "test_imbalanced_style['min_sent_size'] = test_imbalanced_text['sentences'].apply(util.min_sentence_length)\n",
    "test_imbalanced_style[['min_sentence_chars', 'min_sentence_words']] = pd.DataFrame(test_imbalanced_style['min_sent_size'].tolist(), index=test_imbalanced_style.index)\n",
    "test_imbalanced_style['max_sent_size'] = test_imbalanced_text['sentences'].apply(util.max_sentence_length)\n",
    "test_imbalanced_style[['max_sentence_chars', 'max_sentence_words']] = pd.DataFrame(test_imbalanced_style['max_sent_size'].tolist(), index=test_imbalanced_style.index)\n",
    "test_imbalanced_style = test_imbalanced_style.drop(['avg_sent_size', 'std_sent_size', 'min_sent_size', 'max_sent_size'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff0f7d4",
   "metadata": {},
   "source": [
    "## Ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f4baee",
   "metadata": {},
   "source": [
    "There are also the ratios of various text elements (like specific characters to total characters or words to characters) that can be used as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df606b41",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-27T18:49:21.014163Z",
     "end_time": "2023-11-27T18:49:21.307158Z"
    }
   },
   "outputs": [],
   "source": [
    "train_balanced_style['words_to_chars'] = util.series_ratio(train_balanced_style['num_words'], train_balanced_style['num_chars'])\n",
    "train_balanced_style['unique_words_to_words'] = util.series_ratio(train_balanced_style['num_unique_words'], train_balanced_style['num_words'])\n",
    "train_balanced_style['special_chars_to_chars'] = util.series_ratio(train_balanced_style['num_special_chars'], train_balanced_style['num_chars'])\n",
    "\n",
    "train_balanced_style['dots_to_chars'] = train_balanced_text['body'].apply(util.character_to_chars, character='.')\n",
    "train_balanced_style['commas_to_chars'] = train_balanced_text['body'].apply(util.character_to_chars, character=',')\n",
    "train_balanced_style['questionmark_to_chars'] = train_balanced_text['body'].apply(util.character_to_chars, character='?')\n",
    "train_balanced_style['exclamationmark_to_chars'] = train_balanced_text['body'].apply(util.character_to_chars, character='!')\n",
    "\n",
    "train_balanced_style['chars_to_lines'] = train_balanced_text['body'].apply(util.chars_to_lines)\n",
    "train_balanced_style['alpha_tokens_to_words'] = train_balanced_text['body'].apply(util.alpha_tokens_ratio)\n",
    "\n",
    "train_balanced_style['words_to_lines'] = train_balanced_style['words_to_chars'] * train_balanced_style['chars_to_lines']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5c11d32",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-27T18:49:21.310053Z",
     "end_time": "2023-11-27T18:49:21.395127Z"
    }
   },
   "outputs": [],
   "source": [
    "test_balanced_style['words_to_chars'] = util.series_ratio(test_balanced_style['num_words'], test_balanced_style['num_chars'])\n",
    "test_balanced_style['unique_words_to_words'] = util.series_ratio(test_balanced_style['num_unique_words'], test_balanced_style['num_words'])\n",
    "test_balanced_style['special_chars_to_chars'] = util.series_ratio(test_balanced_style['num_special_chars'], test_balanced_style['num_chars'])\n",
    "\n",
    "test_balanced_style['dots_to_chars'] = test_balanced_text['body'].apply(util.character_to_chars, character='.')\n",
    "test_balanced_style['commas_to_chars'] = test_balanced_text['body'].apply(util.character_to_chars, character=',')\n",
    "test_balanced_style['questionmark_to_chars'] = test_balanced_text['body'].apply(util.character_to_chars, character='?')\n",
    "test_balanced_style['exclamationmark_to_chars'] = test_balanced_text['body'].apply(util.character_to_chars, character='!')\n",
    "\n",
    "test_balanced_style['chars_to_lines'] = test_balanced_text['body'].apply(util.chars_to_lines)\n",
    "test_balanced_style['alpha_tokens_to_words'] = test_balanced_text['body'].apply(util.alpha_tokens_ratio)\n",
    "\n",
    "test_balanced_style['words_to_lines'] = test_balanced_style['words_to_chars'] * test_balanced_style['chars_to_lines']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29cce630",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-27T18:49:21.395127Z",
     "end_time": "2023-11-27T18:49:22.966007Z"
    }
   },
   "outputs": [],
   "source": [
    "train_imbalanced_style['words_to_chars'] = util.series_ratio(train_imbalanced_style['num_words'], train_imbalanced_style['num_chars'])\n",
    "train_imbalanced_style['unique_words_to_words'] = util.series_ratio(train_imbalanced_style['num_unique_words'], train_imbalanced_style['num_words'])\n",
    "train_imbalanced_style['special_chars_to_chars'] = util.series_ratio(train_imbalanced_style['num_special_chars'], train_imbalanced_style['num_chars'])\n",
    "\n",
    "train_imbalanced_style['dots_to_chars'] = train_imbalanced_text['body'].apply(util.character_to_chars, character='.')\n",
    "train_imbalanced_style['commas_to_chars'] = train_imbalanced_text['body'].apply(util.character_to_chars, character=',')\n",
    "train_imbalanced_style['questionmark_to_chars'] = train_imbalanced_text['body'].apply(util.character_to_chars, character='?')\n",
    "train_imbalanced_style['exclamationmark_to_chars'] = train_imbalanced_text['body'].apply(util.character_to_chars, character='!')\n",
    "\n",
    "train_imbalanced_style['chars_to_lines'] = train_imbalanced_text['body'].apply(util.chars_to_lines)\n",
    "train_imbalanced_style['alpha_tokens_to_words'] = train_imbalanced_text['body'].apply(util.alpha_tokens_ratio)\n",
    "\n",
    "train_imbalanced_style['words_to_lines'] = train_imbalanced_style['words_to_chars'] * train_imbalanced_style['chars_to_lines']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac7736d9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-27T18:49:22.997138Z",
     "end_time": "2023-11-27T18:49:23.358610Z"
    }
   },
   "outputs": [],
   "source": [
    "test_imbalanced_style['words_to_chars'] = util.series_ratio(test_imbalanced_style['num_words'], test_imbalanced_style['num_chars'])\n",
    "test_imbalanced_style['unique_words_to_words'] = util.series_ratio(test_imbalanced_style['num_unique_words'], test_imbalanced_style['num_words'])\n",
    "test_imbalanced_style['special_chars_to_chars'] = util.series_ratio(test_imbalanced_style['num_special_chars'], test_imbalanced_style['num_chars'])\n",
    "\n",
    "test_imbalanced_style['dots_to_chars'] = test_imbalanced_text['body'].apply(util.character_to_chars, character='.')\n",
    "test_imbalanced_style['commas_to_chars'] = test_imbalanced_text['body'].apply(util.character_to_chars, character=',')\n",
    "test_imbalanced_style['questionmark_to_chars'] = test_imbalanced_text['body'].apply(util.character_to_chars, character='?')\n",
    "test_imbalanced_style['exclamationmark_to_chars'] = test_imbalanced_text['body'].apply(util.character_to_chars, character='!')\n",
    "\n",
    "test_imbalanced_style['chars_to_lines'] = test_imbalanced_text['body'].apply(util.chars_to_lines)\n",
    "test_imbalanced_style['alpha_tokens_to_words'] = test_imbalanced_text['body'].apply(util.alpha_tokens_ratio)\n",
    "\n",
    "test_imbalanced_style['words_to_lines'] = test_imbalanced_style['words_to_chars'] * test_imbalanced_style['chars_to_lines']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be5e9a8",
   "metadata": {},
   "source": [
    "## Readability and Spelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ec2015",
   "metadata": {},
   "source": [
    "Finally, somewhat more advanced readability scores and spelling/grammatical errors can be used as features.<br>\n",
    "Note that the spellcheck is a time consuming procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d5798f4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-27T16:55:57.721437Z",
     "end_time": "2023-11-27T16:56:01.075359Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No java install detected. Please install java to use language-tool-python.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m language_tool \u001B[38;5;241m=\u001B[39m \u001B[43mlanguage_tool_python\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLanguageTool\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43men-US\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcacheSize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpipelineCaching\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaxCheckThreads\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m12\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaxSpellingSuggestions\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\p38phish\\lib\\site-packages\\language_tool_python\\server.py:62\u001B[0m, in \u001B[0;36mLanguageTool.__init__\u001B[1;34m(self, language, motherTongue, remote_server, newSpellings, new_spellings_persist, host, config)\u001B[0m\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_remote_server_config(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_url)\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_server_is_alive():\n\u001B[1;32m---> 62\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_start_server_on_free_port\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m language \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     64\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\.conda\\envs\\p38phish\\lib\\site-packages\\language_tool_python\\server.py:238\u001B[0m, in \u001B[0;36mLanguageTool._start_server_on_free_port\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    236\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttp://\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m/v2/\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_host, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_port)\n\u001B[0;32m    237\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 238\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_start_local_server\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    239\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    240\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ServerError:\n",
      "File \u001B[1;32m~\\.conda\\envs\\p38phish\\lib\\site-packages\\language_tool_python\\server.py:248\u001B[0m, in \u001B[0;36mLanguageTool._start_local_server\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    246\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_start_local_server\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    247\u001B[0m     \u001B[38;5;66;03m# Before starting local server, download language tool if needed.\u001B[39;00m\n\u001B[1;32m--> 248\u001B[0m     \u001B[43mdownload_lt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    249\u001B[0m     err \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    250\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\.conda\\envs\\p38phish\\lib\\site-packages\\language_tool_python\\download_lt.py:144\u001B[0m, in \u001B[0;36mdownload_lt\u001B[1;34m()\u001B[0m\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misdir(download_folder)\n\u001B[0;32m    138\u001B[0m old_path_list \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    139\u001B[0m     path \u001B[38;5;28;01mfor\u001B[39;00m path \u001B[38;5;129;01min\u001B[39;00m\n\u001B[0;32m    140\u001B[0m     glob\u001B[38;5;241m.\u001B[39mglob(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(download_folder, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLanguageTool*\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m    141\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misdir(path)\n\u001B[0;32m    142\u001B[0m ]\n\u001B[1;32m--> 144\u001B[0m \u001B[43mconfirm_java_compatibility\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    145\u001B[0m version \u001B[38;5;241m=\u001B[39m LATEST_VERSION\n\u001B[0;32m    146\u001B[0m filename \u001B[38;5;241m=\u001B[39m FILENAME\u001B[38;5;241m.\u001B[39mformat(version\u001B[38;5;241m=\u001B[39mversion)\n",
      "File \u001B[1;32m~\\.conda\\envs\\p38phish\\lib\\site-packages\\language_tool_python\\download_lt.py:75\u001B[0m, in \u001B[0;36mconfirm_java_compatibility\u001B[1;34m()\u001B[0m\n\u001B[0;32m     70\u001B[0m java_path \u001B[38;5;241m=\u001B[39m find_executable(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjava\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m java_path:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;66;03m# Just ignore this and assume an old version of Java. It might not be\u001B[39;00m\n\u001B[0;32m     73\u001B[0m     \u001B[38;5;66;03m# found because of a PATHEXT-related issue\u001B[39;00m\n\u001B[0;32m     74\u001B[0m     \u001B[38;5;66;03m# (https://bugs.python.org/issue2200).\u001B[39;00m\n\u001B[1;32m---> 75\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNo java install detected. Please install java to use language-tool-python.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     77\u001B[0m output \u001B[38;5;241m=\u001B[39m subprocess\u001B[38;5;241m.\u001B[39mcheck_output([java_path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-version\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m     78\u001B[0m                                  stderr\u001B[38;5;241m=\u001B[39msubprocess\u001B[38;5;241m.\u001B[39mSTDOUT,\n\u001B[0;32m     79\u001B[0m                                  universal_newlines\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     81\u001B[0m major_version, minor_version \u001B[38;5;241m=\u001B[39m parse_java_version(output)\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No java install detected. Please install java to use language-tool-python."
     ]
    }
   ],
   "source": [
    "language_tool = language_tool_python.LanguageTool('en-US', config={'cacheSize': 1000, 'pipelineCaching': True, 'maxCheckThreads': 12, 'maxSpellingSuggestions': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c6a404",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_balanced_style['readability'] = train_balanced_text['body'].apply(util.readability)\n",
    "train_balanced_style[['flesch_kincaid', 'flesch', 'gunning_fog', 'coleman_liau', 'dale_chall', 'automated_readability_index', 'linsear_write', 'spache']] = pd.DataFrame(train_balanced_style['readability'].tolist(), index=train_balanced_style.index)\n",
    "train_balanced_style = train_balanced_style.drop('readability', axis=1)\n",
    "\n",
    "t1 = time()\n",
    "train_balanced_style['errors'] = train_balanced_text['body'].apply(util.errors_check, tool=language_tool)\n",
    "t2 = time()\n",
    "print(\"Time required for balanced train set spellcheck: \", \"{:.2f}\".format(t2-t1), \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828cce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_balanced_style['readability'] = test_balanced_text['body'].apply(util.readability)\n",
    "test_balanced_style[['flesch_kincaid', 'flesch', 'gunning_fog', 'coleman_liau', 'dale_chall', 'automated_readability_index', 'linsear_write', 'spache']] = pd.DataFrame(test_balanced_style['readability'].tolist(), index=test_balanced_style.index)\n",
    "test_balanced_style = test_balanced_style.drop('readability', axis=1)\n",
    "\n",
    "t1 = time()\n",
    "test_balanced_style['errors'] = test_balanced_text['body'].apply(util.errors_check, tool=language_tool)\n",
    "t2 = time()\n",
    "print(\"Time required for balanced test set spellcheck: \", \"{:.2f}\".format(t2-t1), \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcc44a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imbalanced_style['readability'] = train_imbalanced_text['body'].apply(util.readability)\n",
    "train_imbalanced_style[['flesch_kincaid', 'flesch', 'gunning_fog', 'coleman_liau', 'dale_chall', 'automated_readability_index', 'linsear_write', 'spache']] = pd.DataFrame(train_imbalanced_style['readability'].tolist(), index=train_imbalanced_style.index)\n",
    "train_imbalanced_style = train_imbalanced_style.drop('readability', axis=1)\n",
    "\n",
    "t1 = time()\n",
    "train_imbalanced_style['errors'] = train_imbalanced_text['body'].apply(util.errors_check, tool=language_tool)\n",
    "t2 = time()\n",
    "print(\"Time required for imbalanced train set spellcheck: \", \"{:.2f}\".format(t2-t1), \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfe434d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_imbalanced_style['readability'] = test_imbalanced_text['body'].apply(util.readability)\n",
    "test_imbalanced_style[['flesch_kincaid', 'flesch', 'gunning_fog', 'coleman_liau', 'dale_chall', 'automated_readability_index', 'linsear_write', 'spache']] = pd.DataFrame(test_imbalanced_style['readability'].tolist(), index=test_imbalanced_style.index)\n",
    "test_imbalanced_style = test_imbalanced_style.drop('readability', axis=1)\n",
    "\n",
    "t1 = time()\n",
    "test_imbalanced_style['errors'] = test_imbalanced_text['body'].apply(util.errors_check, tool=language_tool)\n",
    "t2 = time()\n",
    "print(\"Time required for imbalanced test set spellcheck: \", \"{:.2f}\".format(t2-t1), \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f945a90",
   "metadata": {},
   "source": [
    "# Final Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1c0c14",
   "metadata": {},
   "source": [
    "The calculated features look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4426907",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_balanced_style.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aadec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_balanced_style.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91196e96",
   "metadata": {},
   "source": [
    "In total, 42 features where created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf16d4d",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ae61fa",
   "metadata": {},
   "source": [
    "There are some missing values in the datasets created, since readability scores do not work for text shorter than 100 words and language tool will stop for very big messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f78a86",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_balanced_style.isna().sum()[train_balanced_style.isna().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf505f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_balanced_style.isna().sum()[test_balanced_style.isna().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39a8264",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_imbalanced_style.isna().sum()[train_imbalanced_style.isna().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db7eeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imbalanced_style.isna().sum()[test_imbalanced_style.isna().any()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b84e90",
   "metadata": {},
   "source": [
    "The missing value in errors can simply be replaced by a big number, for example 2x the time of the maximum value of the column of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5ecaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced_style['errors'] = train_balanced_style['errors'].fillna(train_balanced_style['errors'].max() * 2)\n",
    "train_imbalanced_style['errors'] = train_imbalanced_style['errors'].fillna(train_imbalanced_style['errors'].max() * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f268c12",
   "metadata": {},
   "source": [
    "As for the readability scores, it is obvious it is obvious from the following plots that all scores seem to converge in some value range, while they are unstable when the number of words is close to zero. Using the most frequent value would make sense if the distribution of missing values was random (since it would better approximate the value that the distribution converges to), however the missing values are all close to zero, thus making the use of the mean value the best choice for imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d615f1ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(18, 9))\n",
    "axes[0, 0].scatter(train_imbalanced_style['num_words'], train_imbalanced_style['flesch_kincaid'])\n",
    "axes[0, 0].set_title('flesch_kincaid')\n",
    "axes[0, 1].scatter(train_imbalanced_style['num_words'], train_imbalanced_style['dale_chall'])\n",
    "axes[0, 1].set_title('dale_chall')\n",
    "axes[1, 0].scatter(train_imbalanced_style['num_words'], train_imbalanced_style['gunning_fog'])\n",
    "axes[1, 0].set_title('gunning_fog')\n",
    "axes[1, 1].scatter(train_imbalanced_style['num_words'], train_imbalanced_style['automated_readability_index'])\n",
    "axes[1, 1].set_title('automated_readability_index')\n",
    "plt.setp(axes, xlim=(90, 2000), ylim=(-5, 150))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436cd123",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(18, 9))\n",
    "axes[0, 0].scatter(train_imbalanced_style['num_words'], train_imbalanced_style['flesch'])\n",
    "axes[0, 0].set_title('flesch')\n",
    "axes[0, 1].scatter(train_imbalanced_style['num_words'], train_imbalanced_style['spache'])\n",
    "axes[0, 1].set_title('spache')\n",
    "axes[1, 0].scatter(train_imbalanced_style['num_words'], train_imbalanced_style['linsear_write'])\n",
    "axes[1, 0].set_title('linsear_write')\n",
    "axes[1, 1].scatter(train_imbalanced_style['num_words'], train_imbalanced_style['coleman_liau'])\n",
    "axes[1, 1].set_title('coleman_liau')\n",
    "plt.setp(axes, xlim=(90, 2000), ylim=(-5, 150))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802ba0ac",
   "metadata": {},
   "source": [
    "Thus, the mean value of the training set will be used to replace the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358ae01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced_style, test_balanced_style = impute_missing(train_balanced_style, test_balanced_style, strategy='mean')\n",
    "train_imbalanced_style, test_imbalanced_style = impute_missing(train_imbalanced_style, test_imbalanced_style, strategy='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb873bb0",
   "metadata": {},
   "source": [
    "## Change Column Names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f9f330",
   "metadata": {},
   "source": [
    "For consistency, change the id and class column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1f57be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced_style = train_balanced_style.rename(columns={\"id\": \"email_id\", \"class\": \"email_class\"})\n",
    "test_balanced_style = test_balanced_style.rename(columns={\"id\": \"email_id\", \"class\": \"email_class\"})\n",
    "\n",
    "train_imbalanced_style = train_imbalanced_style.rename(columns={\"id\": \"email_id\", \"class\": \"email_class\"})\n",
    "test_imbalanced_sgtyle = test_imbalanced_style.rename(columns={\"id\": \"email_id\", \"class\": \"email_class\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94f8a74",
   "metadata": {},
   "source": [
    "## Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5035f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_csv(train_balanced_style, csv_path, 'style_train_balanced.csv')\n",
    "save_to_csv(test_balanced_style, csv_path, 'style_test_balanced.csv')\n",
    "\n",
    "save_to_csv(train_imbalanced_style, csv_path, 'style_train_imbalanced.csv')\n",
    "save_to_csv(test_imbalanced_style, csv_path, 'style_test_imbalanced.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
